use ollama_models::{OllamaRequest, OllamaResponse, PerformanceStats};
use std::sync::Arc;
use tokio::sync::RwLock;

/// Ollama API è¯·æ±‚ç»“æ„
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OllamaRequest {
    /// æ¨¡å‹åç§°
    pub model: String,
    /// æç¤ºè¯
    pub prompt: String,
    /// æ˜¯å¦æµå¼è¾“å‡º
    pub stream: bool,
    /// ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¿æŒå¯¹è¯è¿ç»­æ€§ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub context: Option<Vec<i64>>,
}

/// Ollama API å“åº”ç»“æ„
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OllamaResponse {
    /// æ¨¡å‹çš„å›ç­”
    pub response: String,
    /// æ–°çš„ä¼šè¯ä¸Šä¸‹æ–‡
    #[serde(default)]
    pub context: Option<Vec<i64>>,
    /// æ˜¯å¦å®Œæˆ
    #[serde(default)]
    pub done: bool,
    /// å®ŒæˆåŸå› 
    #[serde(default)]
    pub done_reason: Option<String>,
    /// æ¨¡å‹åç§°
    #[serde(default)]
    pub model: Option<String>,
    /// åˆ›å»ºæ—¶é—´
    #[serde(default)]
    pub created_at: Option<String>,
    /// æ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
    #[serde(default)]
    pub thinking: Option<String>,
    /// æ€»å¤„ç†æ—¶é—´ï¼ˆçº³ç§’ï¼‰
    #[serde(default)]
    pub total_duration: Option<u64>,
    /// æ¨¡å‹åŠ è½½æ—¶é—´ï¼ˆçº³ç§’ï¼‰
    #[serde(default)]
    pub load_duration: Option<u64>,
    /// æç¤ºè¯è¯„ä¼°æ¬¡æ•°
    #[serde(default)]
    pub prompt_eval_count: Option<u32>,
    /// æç¤ºè¯è¯„ä¼°æ—¶é—´ï¼ˆçº³ç§’ï¼‰
    #[serde(default)]
    pub prompt_eval_duration: Option<u64>,
    /// ç”Ÿæˆè¯„ä¼°æ¬¡æ•°
    #[serde(default)]
    pub eval_count: Option<u32>,
    /// ç”Ÿæˆè¯„ä¼°æ—¶é—´ï¼ˆçº³ç§’ï¼‰
    #[serde(default)]
    pub eval_duration: Option<u64>,
}

impl OllamaResponse {
    /// è·å–æ€»å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub fn total_duration_ms(&self) -> Option<f64> {
        self.total_duration.map(|ns| ns as f64 / 1_000_000.0)
    }

    /// è·å–æ¨¡å‹åŠ è½½æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub fn load_duration_ms(&self) -> Option<f64> {
        self.load_duration.map(|ns| ns as f64 / 1_000_000.0)
    }

    /// è·å–æç¤ºè¯è¯„ä¼°æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub fn prompt_eval_duration_ms(&self) -> Option<f64> {
        self.prompt_eval_duration.map(|ns| ns as f64 / 1_000_000.0)
    }

    /// è·å–ç”Ÿæˆè¯„ä¼°æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub fn eval_duration_ms(&self) -> Option<f64> {
        self.eval_duration.map(|ns| ns as f64 / 1_000_000.0)
    }

    /// è·å–å¹³å‡æ¯ä¸ªtokençš„ç”Ÿæˆæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub fn avg_eval_time_per_token_ms(&self) -> Option<f64> {
        if let (Some(eval_count), Some(eval_duration)) = (self.eval_count, self.eval_duration) {
            if eval_count > 0 {
                return Some((eval_duration as f64 / eval_count as f64) / 1_000_000.0);
            }
        }
        None
    }

    /// è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    pub fn performance_stats(&self) -> PerformanceStats {
        PerformanceStats {
            total_duration_ms: self.total_duration_ms(),
            load_duration_ms: self.load_duration_ms(),
            prompt_eval_duration_ms: self.prompt_eval_duration_ms(),
            eval_duration_ms: self.eval_duration_ms(),
            prompt_eval_count: self.prompt_eval_count,
            eval_count: self.eval_count,
            avg_eval_time_per_token_ms: self.avg_eval_time_per_token_ms(),
        }
    }

    /// æ£€æŸ¥æ˜¯å¦æœ‰æ€è€ƒè¿‡ç¨‹
    pub fn has_thinking(&self) -> bool {
        self.thinking.as_ref().map_or(false, |t| !t.is_empty())
    }

    /// æ£€æŸ¥æ˜¯å¦æ­£å¸¸å®Œæˆ
    pub fn is_successfully_done(&self) -> bool {
        self.done && self.done_reason.as_ref().map_or(true, |reason| reason == "stop")
    }
}

/// æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct PerformanceStats {
    pub total_duration_ms: Option<f64>,
    pub load_duration_ms: Option<f64>,
    pub prompt_eval_duration_ms: Option<f64>,
    pub eval_duration_ms: Option<f64>,
    pub prompt_eval_count: Option<u32>,
    pub eval_count: Option<u32>,
    pub avg_eval_time_per_token_ms: Option<f64>,
}

impl PerformanceStats {
    /// æ ¼å¼åŒ–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ä¸ºå­—ç¬¦ä¸²
    pub fn format_summary(&self) -> String {
        let mut parts = Vec::new();
        
        if let Some(total) = self.total_duration_ms {
            parts.push(format!("æ€»æ—¶é—´: {:.1}ms", total));
        }
        
        if let Some(load) = self.load_duration_ms {
            parts.push(format!("åŠ è½½: {:.1}ms", load));
        }
        
        if let Some(prompt_eval) = self.prompt_eval_duration_ms {
            parts.push(format!("æç¤ºè¯è¯„ä¼°: {:.1}ms", prompt_eval));
        }
        
        if let Some(eval) = self.eval_duration_ms {
            parts.push(format!("ç”Ÿæˆ: {:.1}ms", eval));
        }
        
        if let Some(count) = self.eval_count {
            parts.push(format!("ç”Ÿæˆtokens: {}", count));
        }
        
        if let Some(avg) = self.avg_eval_time_per_token_ms {
            parts.push(format!("å¹³å‡/token: {:.2}ms", avg));
        }
        
        parts.join(", ")
    }
}

/// Ollama å®¢æˆ·ç«¯
/// 
/// ç”¨äºä¸ Ollama API äº¤äº’ï¼Œæ”¯æŒä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†
/// 
/// # ç¤ºä¾‹
/// 
/// ```rust
/// let client = OllamaClient::new("http://localhost:11434");
/// 
/// // å¼€å§‹æ–°ä¼šè¯
/// let response = client.ask("ä½ å¥½", "gpt-oss:20b", true).await?;
/// println!("å›ç­”: {}", response);
/// 
/// // ç»§ç»­ä¼šè¯ï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰
/// let response2 = client.ask("æˆ‘åˆšæ‰è¯´äº†ä»€ä¹ˆï¼Ÿ", "gpt-oss:20b", false).await?;
/// println!("å›ç­”: {}", response2);
/// 
/// // æ¸…ç©ºä¼šè¯
/// client.clear_context().await;
/// ```
#[derive(Clone)]
pub struct OllamaClient {
    /// Ollama API åŸºç¡€ URL
    base_url: String,
    /// ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    context: Arc<RwLock<Option<Vec<i64>>>>,
    /// HTTP å®¢æˆ·ç«¯
    client: reqwest::Client,
}

impl OllamaClient {
    /// åˆ›å»ºæ–°çš„ Ollama å®¢æˆ·ç«¯
    /// 
    /// # å‚æ•°
    /// 
    /// * `base_url` - Ollama API çš„åŸºç¡€ URLï¼Œä¾‹å¦‚ "http://localhost:11434"
    pub fn new(base_url: impl Into<String>) -> Self {
        Self {
            base_url: base_url.into(),
            context: Arc::new(RwLock::new(None)),
            client: reqwest::Client::new(),
        }
    }

    /// ä»ç¯å¢ƒå˜é‡åˆ›å»º Ollama å®¢æˆ·ç«¯
    /// 
    /// ç¯å¢ƒå˜é‡ï¼š
    /// - `OLLAMA_HOST`: Ollama æœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤: "127.0.0.1"ï¼‰
    /// - `OLLAMA_PORT`: Ollama æœåŠ¡å™¨ç«¯å£ï¼ˆé»˜è®¤: "11434"ï¼‰
    pub fn from_env() -> Self {
        let host = std::env::var("OLLAMA_HOST").unwrap_or_else(|_| "127.0.0.1".to_string());
        let port = std::env::var("OLLAMA_PORT").unwrap_or_else(|_| "11434".to_string());
        let base_url = format!("http://{}:{}", host, port);
        
        Self::new(base_url)
    }

    /// å‘ Ollama æé—®
    /// 
    /// # å‚æ•°
    /// 
    /// * `prompt` - æç¤ºè¯/é—®é¢˜
    /// * `model` - æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ "gpt-oss:20b"
    /// * `new_session` - æ˜¯å¦å¼€å§‹æ–°ä¼šè¯ï¼ˆtrue ä¼šæ¸…ç©ºä¸Šä¸‹æ–‡ï¼‰
    /// 
    /// # è¿”å›
    /// 
    /// è¿”å›æ¨¡å‹çš„å›ç­”å­—ç¬¦ä¸²
    /// 
    /// # é”™è¯¯
    /// 
    /// å¦‚æœè¯·æ±‚å¤±è´¥æˆ–è§£æå“åº”å¤±è´¥ï¼Œè¿”å›é”™è¯¯
    pub async fn ask(
        &self,
        prompt: impl Into<String>,
        model: impl Into<String>,
        new_session: bool,
    ) -> Result<String, Box<dyn std::error::Error>> {
        // å¦‚æœæ˜¯æ–°ä¼šè¯ï¼Œæ¸…ç©º context
        if new_session {
            let mut context_guard = self.context.write().await;
            *context_guard = None;
            log::info!("ğŸ”„ æ¸…ç©º Ollama ä¼šè¯ä¸Šä¸‹æ–‡");
        }

        // è¯»å–å½“å‰ context
        let current_context = {
            let context_guard = self.context.read().await;
            context_guard.clone()
        };

        let prompt_str = prompt.into();
        let model_str = model.into();

        log::info!("ğŸ“¤ å‘é€ Ollama è¯·æ±‚ - æ¨¡å‹: {}, æç¤ºè¯é•¿åº¦: {}", 
                   model_str, prompt_str.len());

        // æ„é€ è¯·æ±‚
        let request = OllamaRequest {
            model: model_str,
            prompt: prompt_str,
            stream: false,
            context: current_context,
        };

        // å‘é€è¯·æ±‚
        let response = self
            .client
            .post(format!("{}/api/generate", self.base_url))
            .json(&request)
            .send()
            .await?
            .json::<OllamaResponse>()
            .await?;

        log::info!("ğŸ“¥ æ”¶åˆ° Ollama å“åº” - å›ç­”é•¿åº¦: {}", response.response.len());

        // æ›´æ–° context
        if let Some(new_context) = response.context.clone() {
            let mut context_guard = self.context.write().await;
            *context_guard = Some(new_context.clone());
            log::debug!("ğŸ’¾ ä¿å­˜ä¼šè¯ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {}", new_context.len());
        }

        Ok(response.response)
    }

    /// å‘ Ollama æé—®ï¼ˆä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼‰
    /// 
    /// é»˜è®¤æ¨¡å‹: "gpt-oss:20b"
    #[allow(dead_code)]
    pub async fn ask_default(
        &self,
        prompt: impl Into<String>,
        new_session: bool,
    ) -> Result<String, Box<dyn std::error::Error>> {
        self.ask(prompt, "gpt-oss:20b", new_session).await
    }

    /// è·å–å®Œæ•´çš„å“åº”å¯¹è±¡ï¼ˆåŒ…å«æ‰€æœ‰å…ƒæ•°æ®ï¼‰
    #[allow(dead_code)]
    pub async fn ask_full(
        &self,
        prompt: impl Into<String>,
        model: impl Into<String>,
        new_session: bool,
    ) -> Result<OllamaResponse, Box<dyn std::error::Error>> {
        // å¦‚æœæ˜¯æ–°ä¼šè¯ï¼Œæ¸…ç©º context
        if new_session {
            let mut context_guard = self.context.write().await;
            *context_guard = None;
        }

        // è¯»å–å½“å‰ context
        let current_context = {
            let context_guard = self.context.read().await;
            context_guard.clone()
        };

        // æ„é€ è¯·æ±‚
        let request = OllamaRequest {
            model: model.into(),
            prompt: prompt.into(),
            stream: false,
            context: current_context,
        };

        // å‘é€è¯·æ±‚
        let response = self
            .client
            .post(format!("{}/api/generate", self.base_url))
            .json(&request)
            .send()
            .await?
            .json::<OllamaResponse>()
            .await?;

        // æ›´æ–° context
        if let Some(new_context) = response.context.clone() {
            let mut context_guard = self.context.write().await;
            *context_guard = Some(new_context);
        }

        Ok(response)
    }

    /// æ¸…ç©ºä¼šè¯ä¸Šä¸‹æ–‡
    pub async fn clear_context(&self) {
        let mut context_guard = self.context.write().await;
        *context_guard = None;
        log::info!("ğŸ—‘ï¸  æ¸…ç©º Ollama ä¼šè¯ä¸Šä¸‹æ–‡");
    }

    /// è·å–å½“å‰ä¼šè¯ä¸Šä¸‹æ–‡
    #[allow(dead_code)]
    pub async fn get_context(&self) -> Option<Vec<i64>> {
        let context_guard = self.context.read().await;
        context_guard.clone()
    }

    /// æ£€æŸ¥æ˜¯å¦æœ‰æ´»åŠ¨çš„ä¼šè¯ä¸Šä¸‹æ–‡
    pub async fn has_context(&self) -> bool {
        let context_guard = self.context.read().await;
        context_guard.is_some()
    }

    /// è·å–ä¸Šä¸‹æ–‡å¤§å°
    pub async fn context_size(&self) -> usize {
        let context_guard = self.context.read().await;
        context_guard.as_ref().map_or(0, |ctx| ctx.len())
    }
}

impl Default for OllamaClient {
    fn default() -> Self {
        Self::new("http://localhost:11434")
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_ollama_client_creation() {
        let client = OllamaClient::new("http://localhost:11434");
        assert!(!client.has_context().await);
        assert_eq!(client.context_size().await, 0);
    }

    #[tokio::test]
    async fn test_clear_context() {
        let client = OllamaClient::new("http://localhost:11434");
        client.clear_context().await;
        assert!(!client.has_context().await);
    }
}

